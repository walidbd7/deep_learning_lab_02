{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "train_set = requests.get(\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a9a\")\n",
    "val_set = requests.get(\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a9a.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "x_train, y_train = load_svmlight_file(BytesIO(train_set.content), n_features=123)\n",
    "x_val, y_val = load_svmlight_file(BytesIO(val_set.content), n_features=123)\n",
    "x_train = x_train.toarray()\n",
    "x_val = x_val.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from collections import defaultdict\n",
    "\n",
    "n_samples_train, n_features_train = x_train.shape\n",
    "x_train = numpy.concatenate((x_train, numpy.ones(shape=(n_samples_train, 1))), axis=1)\n",
    "y_train = y_train.reshape((n_samples_train, 1))\n",
    "\n",
    "n_samples_val, n_features_val = x_val.shape\n",
    "x_val = numpy.concatenate((x_val, numpy.ones(shape=(n_samples_val, 1))), axis=1)\n",
    "y_val = y_val.reshape((n_samples_val, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, n_features):\n",
    "        self.params = numpy.random.random(size=(n_features, 1))\n",
    "        self.diffs = numpy.zeros((n_features, 1))\n",
    "        self.recorder = defaultdict(list)\n",
    "        \n",
    "    def train(self, x, y):\n",
    "        pass\n",
    "\n",
    "    def validate(self, x, y):\n",
    "        self.__loss__(x, y, \"validation\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        pass\n",
    "\n",
    "    def __calculate_gradient__(self, params=None):\n",
    "        pass\n",
    "\n",
    "    def __loss__(self, x, y, key):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(Model):\n",
    "    def __init__(self, n_features):\n",
    "        super(LogisticRegression, self).__init__(n_features=n_features)\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def train(self, x, y):\n",
    "        self.x_train = x\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, x):\n",
    "        return numpy.where(numpy.dot(x, self.params) > 0, 1, 0)\n",
    "\n",
    "    def __calculate_gradient__(self, params=None):\n",
    "        if params is None:\n",
    "            params = self.params\n",
    "        y_hat = 1 / (1 + numpy.exp(-numpy.dot(self.x_train, params)))\n",
    "        self.diffs = numpy.dot(self.x_train.transpose(), (y_hat - self.y_train))\n",
    "\n",
    "    def __loss__(self, x, y, key):\n",
    "        y_hat = 1 / (1 + numpy.exp(-numpy.dot(x, self.params)))\n",
    "        loss = -numpy.average(y * numpy.log(y_hat) + (1 - y) * numpy.log(1 - y_hat))\n",
    "        self.recorder[key].append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.color = None\n",
    "\n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, learning_rate, momentum=None):\n",
    "        super(SGD, self).__init__(model=model)\n",
    "        self.color = \"r\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        if momentum is not None:\n",
    "            self.v = numpy.zeros_like(self.model.diffs)\n",
    "\n",
    "    def step(self):\n",
    "        self.model.__calculate_gradient__()\n",
    "        if self.momentum is None:\n",
    "            self.model.params -= self.learning_rate * self.model.diffs\n",
    "        else:\n",
    "            self.v = self.momentum * self.v + self.learning_rate * self.model.diffs\n",
    "            self.model.params -= self.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAG(Optimizer):\n",
    "    def __init__(self, model, learning_rate, momentum):\n",
    "        super(NAG, self).__init__(model=model)\n",
    "        self.color = \"y\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = numpy.zeros_like(self.model.diffs)\n",
    "    def step(self):\n",
    "        self.model.__calculate_gradient__(params=self.model.params - self.momentum * self.v)\n",
    "        self.v = self.momentum * self.v + self.learning_rate * self.model.diffs\n",
    "        self.model.params -= self.v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, beta, gamma, eta):\n",
    "        super(Adam, self).__init__(model=model)\n",
    "        self.color = \"g\"\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.eta = eta\n",
    "        self.m = numpy.zeros_like(self.model.diffs)\n",
    "        self.G = numpy.zeros_like(self.model.diffs)\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def step(self):\n",
    "        self.model.__calculate_gradient__()\n",
    "        self.m = self.beta * self.m + (1 - self.beta) * self.model.diffs\n",
    "        self.G = self.gamma * self.G + (1 - self.gamma) * self.model.diffs * self.model.diffs\n",
    "        alpha = self.eta * (numpy.sqrt(1 - self.gamma)) / (1 - self.beta)\n",
    "        self.model.params -= alpha * self.m / numpy.sqrt(self.G + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaDelta(Optimizer):\n",
    "    def __init__(self, model, gamma):\n",
    "        super(AdaDelta, self).__init__(model=model)\n",
    "        self.color = \"b\"\n",
    "        self.gamma = gamma\n",
    "        self.G = numpy.zeros_like(self.model.diffs)\n",
    "        self.delta = numpy.zeros_like(self.model.diffs)\n",
    "        self.delta_theta = numpy.zeros_like(self.model.diffs)\n",
    "        self.epsilon = 1e-4\n",
    "\n",
    "    def step(self):\n",
    "        self.model.__calculate_gradient__()\n",
    "        self.G = self.gamma * self.G + (1 - self.gamma) * self.model.diffs * self.model.diffs\n",
    "        self.delta_theta = -(numpy.sqrt(self.delta + self.epsilon)\n",
    "                             / numpy.sqrt(self.G + self.epsilon)) * self.model.diffs\n",
    "        self.model.params += self.delta_theta\n",
    "        self.delta = self.gamma * self.delta + (1 - self.gamma) * self.delta_theta * self.delta_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RMSProP(Optimizer):\n",
    "    def __init__(self, model, leaning_rate, weight_decay):\n",
    "        self.color = \"c\"\n",
    "        super(RMSProP, self).__init__(model=model)\n",
    "        self.G = numpy.zeros_like(self.model.diffs)\n",
    "        self.learning_rate = leaning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def step(self):\n",
    "        self.model.__calculate_gradient__()\n",
    "        self.G = self.weight_decay * self.G + (1 - self.weight_decay) * self.model.diffs * self.model.diffs\n",
    "        self.model.params -= self.learning_rate / numpy.sqrt(self.G + self.epsilon) * self.model.diffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = numpy.where(y_train == -1, 0, y_train)\n",
    "y_val = numpy.where(y_val == -1, 0, y_val)\n",
    "optimizers = [\n",
    "    SGD(model=LogisticRegression(n_features=123 + 1), learning_rate=0.00001, momentum=0.5),\n",
    "    NAG(model=LogisticRegression(n_features=123 + 1), learning_rate=0.00001, momentum=0.5),\n",
    "    Adam(model=LogisticRegression(n_features=123 + 1), beta=0.9, gamma=0.999, eta=0.1),\n",
    "    AdaDelta(model=LogisticRegression(n_features=123 + 1), gamma=0.95),\n",
    "    RMSProP(model=LogisticRegression(n_features=123 + 1), leaning_rate=0.1, weight_decay=0.9)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 100\n",
    "batch_size = 10000\n",
    "for epoch in range(max_epoch):\n",
    "    indexes = numpy.random.randint(0, n_samples_train, size=batch_size)\n",
    "    for optimizer in optimizers:\n",
    "        optimizer.model.train(x_train[indexes], y_train[indexes])\n",
    "        optimizer.step()\n",
    "        optimizer.model.validate(x_val, y_val)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\t\\t\\t\"+optimizers[0].model.__class__.__name__)\n",
    "print(\"-\"*60)\n",
    "for optimizer in optimizers:\n",
    "    print(\"\\t\\t\\t\"+optimizer.__class__.__name__)\n",
    "    print(\"-\"*60)\n",
    "    print(classification_report(y_val,optimizer.model.predict(x_val),target_names=[\"positive\", \"negative\"],digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Logistic Regression and Stocastic Gradient Descent\")\n",
    "for optimizer in optimizers:\n",
    "    plt.plot(optimizer.model.recorder[\"validation\"], color=optimizer.color, label=optimizer.__class__.__name__)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
